{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOCyJn+ikx7HJe1Qp8Bx7Pl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leman-cap13/Machine_Learning/blob/projects/TokenClassificationHuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk2byEzg-tsV"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets=load_dataset('conll2003')"
      ],
      "metadata": {
        "id": "ea7ui3qT-3B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train']"
      ],
      "metadata": {
        "id": "c1LBFiX5_bqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'][0]"
      ],
      "metadata": {
        "id": "mk9Yu4jo_l9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_feature=raw_datasets['train'].features['ner_tags']\n",
        "ner_feature"
      ],
      "metadata": {
        "id": "p7-3wPDK_o5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_names=ner_feature.feature.names\n",
        "label_names"
      ],
      "metadata": {
        "id": "ffCPtEF__2p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=raw_datasets['train'][0]['tokens']\n",
        "labels=raw_datasets['train'][0]['ner_tags']\n",
        "\n",
        "line1=''\n",
        "line2=''\n",
        "\n",
        "for  word, label in zip(words,labels):\n",
        "  full_label=label_names[label]\n",
        "  max_length=max(len(word),len(full_label))\n",
        "  line1+=word+' ' * (max_length-len(word)+1)\n",
        "  line2+=full_label+ ' '* (max_length- len(full_label)+1)\n",
        "print(line1)\n",
        "print(line2)\n"
      ],
      "metadata": {
        "id": "unNljqDYANHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint='bert-base-cased'\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "S76Sn4WeAPaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.is_fast"
      ],
      "metadata": {
        "id": "ywsTOgKXAPXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=tokenizer(raw_datasets['train'][0]['tokens'], is_split_into_words=True)\n",
        "print(inputs)\n",
        "print(inputs.tokens())\n",
        "print(inputs.word_ids())"
      ],
      "metadata": {
        "id": "Vb5WAmk7BRfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def align_labels_with_tokens(labels,word_ids):\n",
        "  new_labels=[]\n",
        "  current_word=None\n",
        "  for word_id in word_ids:\n",
        "    if word_id != current_word:\n",
        "      current_word=word_id\n",
        "      label=-100 if word_id is None else labels[word_id]\n",
        "      new_labels.append(label)\n",
        "    elif word_id is None:\n",
        "      new_labels.append(-100)\n",
        "    else:\n",
        "      label=labels[word_id]\n",
        "      if label % 2==1:\n",
        "        label+=1\n",
        "      new_labels.append(label)\n",
        "  return new_labels\n"
      ],
      "metadata": {
        "id": "NgzG4kTVBRb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = raw_datasets['train'][0]['ner_tags']\n",
        "word_ids=inputs.word_ids()\n",
        "print(labels)\n",
        "print(align_labels_with_tokens(labels,word_ids))"
      ],
      "metadata": {
        "id": "804UCnm0DRzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "  tokenized_inputs=tokenizer(examples['tokens'],truncation=True,is_split_into_words=True)\n",
        "  all_labels=examples['ner_tags']\n",
        "  new_labels=[]\n",
        "  for i , labels in enumerate(all_labels):\n",
        "    word_ids=tokenized_inputs.word_ids(i)\n",
        "    new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
        "  tokenized_inputs['labels'] = new_labels\n",
        "  return tokenized_inputs"
      ],
      "metadata": {
        "id": "MOwTkVk-DRv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets= raw_datasets.map(tokenize_and_align_labels, batched=True, remove_columns=raw_datasets['train'].column_names)"
      ],
      "metadata": {
        "id": "5ES67nC0DRtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "yMKVbxXADRqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#collator sadece imputu padding edi ama collator for classification hem imputu hem outputu pad edir"
      ],
      "metadata": {
        "id": "atE-eI97JDX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "2pkEPQfmJDUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch=data_collator([tokenized_datasets['train'][i] for i in range(2)])\n",
        "batch['labels']"
      ],
      "metadata": {
        "id": "g5xP_tk4J1_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2):\n",
        "  print(tokenized_datasets['train'][i]['labels'])"
      ],
      "metadata": {
        "id": "V4lrqrhbKEVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n"
      ],
      "metadata": {
        "id": "bADvydZ2KERt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "id": "8W-OdY3sMh_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metrics=evaluate.load('seqeval')"
      ],
      "metadata": {
        "id": "F1Puj3sAKMzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels=raw_datasets['train'][0]['ner_tags']\n",
        "labels=[label_names[i] for i in labels]\n",
        "labels"
      ],
      "metadata": {
        "id": "JwWVaNXdKMv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions=labels.copy()\n",
        "predictions[2]='O'\n",
        "metrics.compute(predictions=[predictions],references=[labels])"
      ],
      "metadata": {
        "id": "h_BpXI9pKRgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "012oITJFNmt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "  logits,labels=eval_preds\n",
        "  predictions=np.argmax(logits,axis=-1)\n",
        "  true_labels=[[label_names[l] for l in label if l != -100] for label in labels]\n",
        "  true_predictions=[\n",
        "      [label_names[p] for (p,l) in zip(prediction,label) if l != -100 ] for prediction, label in zip(predictions, labels)\n",
        "  ]\n",
        "\n",
        "  all_metrics=metrics.compute(predictions=true_predictions, references=true_labels)\n",
        "  return {\n",
        "      'precision' : all_metrics['overall_precision'],\n",
        "      'recall' : all_metrics['overall_recall'],\n",
        "      'f1' : all_metrics['overall_f1'],\n",
        "      'accuracy' : all_metrics['overall_accuracy'],\n",
        "  }\n"
      ],
      "metadata": {
        "id": "Io0hOlFpNOHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label={i: label for i, label in enumerate(label_names)}\n",
        "label2id={v:k for k, v in id2label.items()}"
      ],
      "metadata": {
        "id": "KsjXcBOvNOBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label"
      ],
      "metadata": {
        "id": "CVLXQsj9PmBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label2id"
      ],
      "metadata": {
        "id": "uSMNAT3CPorc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model=AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")"
      ],
      "metadata": {
        "id": "JlOdmKrbPon4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "ciV-Gc9CPolV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args=TrainingArguments(\n",
        "    'bert-finetuned-ner',\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "TmaKkWATRJr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer=Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    processing_class=tokenizer\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "M8fSXyTpRpa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(commit_message='Training complete')"
      ],
      "metadata": {
        "id": "mytSg-D3SWKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A custom training loop"
      ],
      "metadata": {
        "id": "fmcWR9LIUZ3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader=DataLoader(\n",
        "    tokenized_datasets['train'],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8\n",
        ")\n",
        "eval_dataloader=DataLoader(\n",
        "    tokenized_datasets['validation'],\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8\n",
        ")\n"
      ],
      "metadata": {
        "id": "LIpWYgFZWp3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")"
      ],
      "metadata": {
        "id": "wm8apFJcWp0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer=AdamW(model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "id": "d52xdAq9Xyc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator=Accelerator()\n",
        "\n",
        "model,optimizer,train_dataloader, eval_dataloader=accelerator.prepare(\n",
        "    model,optimizer, train_dataloader, eval_dataloader\n",
        ")\n",
        "\n",
        "#accelerate suretlendirirn tensorda ehtiyac yoxdu ozu edir biz paytorcda ozumuz secirdik chu yosa gpu olsun deye ama tensor ozu avtomotic edirdi\n"
      ],
      "metadata": {
        "id": "6Chz4mD-XyZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_train_epochs=3\n",
        "num_update_steps_per_epoch=len(train_dataloader)\n",
        "num_training_steps=num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "lr_scheduler=get_scheduler(\n",
        "    'linear',\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps\n",
        ")"
      ],
      "metadata": {
        "id": "altXniaFaMUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import Repository, get_full_repo_name\n",
        "\n",
        "model_name='bert-finetuned-ner-accelerate'\n",
        "repo_name=get_full_repo_name(model_name)\n",
        "repo_name"
      ],
      "metadata": {
        "id": "RhQsfaTCaMRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo\n",
        "\n",
        "repo=create_repo(repo_name, exist_ok=True)"
      ],
      "metadata": {
        "id": "Vt6WH9p1cKfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir='bert-finetuned-ner-accelerate'\n",
        "\n",
        "repo=Repository(output_dir,clone_from=repo_name)\n",
        "\n",
        "# Repository push to hugging face low level formasidir"
      ],
      "metadata": {
        "id": "-fQ2MFJ7biGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess(predictions,labels):\n",
        "  predictions=predictions.detach().cpu().clone().numpy()\n",
        "  labels=labels.detach().cpu().clone().numpy()\n",
        "\n",
        "  true_labels=[[label_names[l] for l in label if l != -100]\n",
        "               for label in labels]\n",
        "\n",
        "  true_predictions=[\n",
        "      [label_names[p] for (p,l) in zip(prediction, label) if l !=-100]\n",
        "      for prediction, label in zip(predictions, labels)\n",
        "  ]\n",
        "  return true_labels, true_predictions"
      ],
      "metadata": {
        "id": "qIAI8ZJebiDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what is postprocess and pre process?"
      ],
      "metadata": {
        "id": "L3IOLQ4pccT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "\n",
        "progress_bar=tqdm(range(num_training_steps))\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "  model.train()\n",
        "  for batch in train_dataloader:\n",
        "    outputs=model(**batch)\n",
        "    loss=outputs.loss\n",
        "    accelerator.backward(loss)\n",
        "\n",
        "    optimizer.step()\n",
        "    lr_scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    progress_bar.update(1)\n",
        "\n",
        "  model.eval()\n",
        "  for batch in eval_dataloader:\n",
        "    with torch.no_grad():\n",
        "      outputs=model(**batch)\n",
        "    predictions=outputs.logits.argmax(dim=-1)\n",
        "    labels=batch['labels']\n",
        "\n",
        "    predictions=accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
        "    labels=accelerator.pad_across_processes(labels,dim=1, pad_index=-100)\n",
        "\n",
        "    predictions_gathered=accelerator.gather(predictions)\n",
        "    labels_gathered=accelerator.gather(labels)\n",
        "\n",
        "    true_predictions, true_labels=postprocess(predictions_gathered,labels_gathered)\n",
        "    metrics.add_batch(predictions=true_predictions, references=true_labels)\n",
        "\n",
        "  result=metrics.compute()\n",
        "  print(\n",
        "      f\"epoch{epoch}:\",\n",
        "      {\n",
        "          key: result[f\"overall_{key}\"]\n",
        "          for key in ['precision','recall','f1','accuracy']\n",
        "      }\n",
        "  )\n",
        "\n",
        "  accelerator.wait_for_everyone()\n",
        "  unwrapped_model=accelerator.unwrap_model(model)\n",
        "  unwrapped_model.save_pretrained(output_dir)\n",
        "  if accelerator.is_main_process:\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    repo.push_to_hub(\n",
        "        commit_message=f\"Traning in progress epoch { epoch}\", blocking=False\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "dYGHon2BccQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradientlerin hesablanmasi dropout batch normalization zamani train de ayri tesde ayri emeliyyatlar olur"
      ],
      "metadata": {
        "id": "XlauC7zXdchL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nk1SSeEPdcdx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}